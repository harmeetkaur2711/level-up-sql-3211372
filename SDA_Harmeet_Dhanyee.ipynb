{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNyc5Ds5w4O7n4j9qeBX+x/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harmeetkaur2711/level-up-sql-3211372/blob/main/SDA_Harmeet_Dhanyee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JssAdai4ZUVt"
      },
      "source": [
        "# **Assignment for Senior Data Analyst Role**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHVncx0CjZXj",
        "outputId": "f48e4222-6295-405d-e385-27465cac555d"
      },
      "source": [
        "# Mounting google drive to access file from the drive directly\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOjSC02dZxLE"
      },
      "source": [
        "### **Installing all the necessary libraries and importing them**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpHAHscMR7hd",
        "outputId": "8c452646-e026-42d2-e302-c06e2d08b0f7"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Important library for many geopython libraries\n",
        "!apt install gdal-bin python-gdal python3-gdal\n",
        "# Install rtree - Geopandas requirment\n",
        "!apt install python3-rtree\n",
        "# Install Geopandas\n",
        "!pip install git+git://github.com/geopandas/geopandas.git\n",
        "# Install descartes - Geopandas requirment\n",
        "!pip install descartes\n",
        "# Install Folium for Geographic data visualization\n",
        "!pip install folium\n",
        "# Install plotlyExpress\n",
        "!pip install plotly_express\n",
        "# install Basemap\n",
        "!apt-get install libgeos-3.5.0\n",
        "!apt-get install libgeos-dev\n",
        "!pip install https://github.com/matplotlib/basemap/archive/master.zip\n",
        "# Installing Chart_Studio\n",
        "!pip install chart_studio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package python-gdal is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  gdal-bin\n",
            "\n",
            "\u001b[1;31mE: \u001b[0mPackage 'python-gdal' has no installation candidate\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libspatialindex-c6 libspatialindex-dev libspatialindex6\n",
            "The following NEW packages will be installed:\n",
            "  libspatialindex-c6 libspatialindex-dev libspatialindex6 python3-rtree\n",
            "0 upgraded, 4 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 365 kB of archives.\n",
            "After this operation, 1,799 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libspatialindex6 amd64 1.9.3-2 [247 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libspatialindex-c6 amd64 1.9.3-2 [55.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libspatialindex-dev amd64 1.9.3-2 [16.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-rtree all 0.9.7-1 [46.4 kB]\n",
            "Fetched 365 kB in 0s (1,244 kB/s)\n",
            "Selecting previously unselected package libspatialindex6:amd64.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../libspatialindex6_1.9.3-2_amd64.deb ...\n",
            "Unpacking libspatialindex6:amd64 (1.9.3-2) ...\n",
            "Selecting previously unselected package libspatialindex-c6:amd64.\n",
            "Preparing to unpack .../libspatialindex-c6_1.9.3-2_amd64.deb ...\n",
            "Unpacking libspatialindex-c6:amd64 (1.9.3-2) ...\n",
            "Selecting previously unselected package libspatialindex-dev:amd64.\n",
            "Preparing to unpack .../libspatialindex-dev_1.9.3-2_amd64.deb ...\n",
            "Unpacking libspatialindex-dev:amd64 (1.9.3-2) ...\n",
            "Selecting previously unselected package python3-rtree.\n",
            "Preparing to unpack .../python3-rtree_0.9.7-1_all.deb ...\n",
            "Unpacking python3-rtree (0.9.7-1) ...\n",
            "Setting up libspatialindex6:amd64 (1.9.3-2) ...\n",
            "Setting up libspatialindex-c6:amd64 (1.9.3-2) ...\n",
            "Setting up libspatialindex-dev:amd64 (1.9.3-2) ...\n",
            "Setting up python3-rtree (0.9.7-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Collecting git+git://github.com/geopandas/geopandas.git\n",
            "  Cloning git://github.com/geopandas/geopandas.git to /tmp/pip-req-build-lbrn4lh5\n",
            "  Running command git clone --filter=blob:none --quiet git://github.com/geopandas/geopandas.git /tmp/pip-req-build-lbrn4lh5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uEZQiDFeqbh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "c43d9caf-2e25-4a1b-c379-df4ce522f596"
      },
      "source": [
        "# importing libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, Polygon\n",
        "import folium\n",
        "import plotly_express as px\n",
        "from geopy.geocoders import Nominatim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "import chart_studio.plotly as py\n",
        "import plotly.graph_objs as go #importing graphical objects\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mpl_toolkits.basemap'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-391799d23ea3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchart_studio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotly\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m \u001b[0;31m#importing graphical objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpl_toolkits.basemap'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAjVIV5IaJkG"
      },
      "source": [
        "## **Importing the Parking Tags Data and combining them into one dataframe**\n",
        "\n",
        "Please note that I have taken data for last 3 years only, so that the system does not get sluggish, but if required the same analysis can be performed on any year of data by loading appropriate files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcv3Cml1SXdC"
      },
      "source": [
        "path = r'/content/drive/My Drive/SDA' # use your path\n",
        "all_files = glob.glob(path + \"/Parking*.csv\")\n",
        "\n",
        "li = []\n",
        "\n",
        "for filename in all_files:\n",
        "    frame = pd.read_csv(filename, index_col=None, header=0)\n",
        "    li.append(frame)\n",
        "\n",
        "df = pd.concat(li, axis=0, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxCH2s1kbfKu"
      },
      "source": [
        "### **Viewing Parking Data and Performing Initial Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DL1NsahgLct"
      },
      "source": [
        "#Viewing data\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8xI_emwTAm6"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-GSo4nHBZ9T"
      },
      "source": [
        "### Performing Data Cleaning for Parking Infraction data\n",
        "\n",
        "Lets first view province data to check if we have data for only one province or more provinces in Cananda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxIC1Jzzb4iX"
      },
      "source": [
        "df.province.value_counts().head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBiSFI31CuLa"
      },
      "source": [
        "# Viewing percentage of rows that contain data of Ontario\n",
        "df.province[df.province=='ON'].value_counts()/df.province.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5BQt2zMhSTH"
      },
      "source": [
        "From above analysis we can see that the licesnce plate of the cars majorly belonged to Ontario however around 4% of them are from different states of Canada and US"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V50mr9ziEFAp"
      },
      "source": [
        "#### Checking Location Data\n",
        "\n",
        "From the table we can see that Location2 caontains the main street address where the infraction has happened, and location3 denotes how location 2 is related to location 4.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTy4lCnEb4b0"
      },
      "source": [
        "# Checking values of location3 to see if they are consistent\n",
        "df.location3.value_counts().head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqwDWmVVkQY5"
      },
      "source": [
        "# Replacing differently spelled codes with one code\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "df.location3.replace(dict.fromkeys(['WO','W OF','W/OF','WEST OF','WEST/OF','W/S','W O','WS'], 'W/O'), inplace=True)\n",
        "df.location3.replace(dict.fromkeys(['EO','E OF','E/OF','EAST OF','EAST/OF','E/S','E O','ES'], 'E/O'), inplace=True)\n",
        "df.location3.replace(dict.fromkeys(['NO','N OF','N/OF','NORTH OF','NORTH/OF','N/S','N O','NS'], 'N/O'), inplace=True)\n",
        "df.location3.replace(dict.fromkeys(['SO','S OF','S/OF','SOUTH OF','SOUTH/OF','S/S','S O','SS'], 'S/O'), inplace=True)\n",
        "df.location3.replace(dict.fromkeys(['AT','ON','OF','NEAR','NR','ACROSS'], 'AND'), inplace=True)\n",
        "df.location3.value_counts().head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miv8sZwsoLF1"
      },
      "source": [
        "# Replacing all non-consistent data with blank\n",
        "X = ['W/O','E/O','N/O','S/O','AND']\n",
        "df.loc[~df.location3.isin(X), 'location3'] = ''\n",
        "df.location3.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju7rfnycHczX"
      },
      "source": [
        "# Replacing NaN values with blanks for Location4 as well\n",
        "df.location4.replace(np.nan,'',inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEPfjzMF-cg"
      },
      "source": [
        "#### Converting date of infraction from integer value to proper datetime format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_LSSPI0b4WF"
      },
      "source": [
        "df['date_of_infraction']=pd.to_datetime(df.date_of_infraction, format='%Y%m%d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX41MLifb4Td"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxYrt1XLhvPl"
      },
      "source": [
        "## Viewing Green Park Data\n",
        "\n",
        "This is a Json file that contains information of all Green Parking locations around Toronto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq8aB40WlniG"
      },
      "source": [
        "# importing file\n",
        "import json\n",
        "with open('/content/drive/MyDrive/SDA/green-p-parking-2019.json') as f:\n",
        "  temp_view = json.load(f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXne2Hu0mfC1"
      },
      "source": [
        "# Viewing file\n",
        "print(temp_view)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCnBbzqFoq7Q"
      },
      "source": [
        "# Viewing data for one Green Park to see what kind of information is available\n",
        "print(json.dumps(temp_view['carparks'][1], indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVCgPasfijQB"
      },
      "source": [
        "### Performing Data Cleaning for Green Park Data\n",
        "\n",
        "Now that we know what kind of information is available in the Json we will try to retrive this information into a pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3qAiDQ3t07m"
      },
      "source": [
        "# Reading the json file and converting it into pandas Dataframe\n",
        "green_park = pd.read_json('/content/drive/MyDrive/SDA/green-p-parking-2019.json')\n",
        "green_park"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4al78SL1xcA"
      },
      "source": [
        "# Retrieving Necessary information into columns of Pandas Dataframe\n",
        "green_park['id'] = green_park.apply(lambda x: json.dumps(x['carparks']['id']).strip('\\\"'), axis=1)\n",
        "green_park['lat'] = green_park.apply(lambda x: json.dumps(x['carparks']['lat']).strip('\\\"'), axis=1)\n",
        "green_park['lng'] = green_park.apply(lambda x: json.dumps(x['carparks']['lng']).strip('\\\"'), axis=1)\n",
        "green_park['address'] = green_park.apply(lambda x: json.dumps(x['carparks']['address']).strip('\\\"'), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poTe2du6uBdx"
      },
      "source": [
        "#Viewing data\n",
        "green_park"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fzY54uPjXvT"
      },
      "source": [
        "## Viewing TTC locations data\n",
        "\n",
        "We dont need much data cleaning as we mostly require only latitude and longitude data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-osrx8ZuBRr"
      },
      "source": [
        "#Reading required file\n",
        "ttc = pd.read_csv('/content/drive/My Drive/SDA/stops.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-gru9EERScS"
      },
      "source": [
        "# Viewing Data\n",
        "ttc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6tuxGbHILba"
      },
      "source": [
        "## Performing Data Analysis and Finding answers to the query\n",
        "\n",
        "Now that our data is in better shape we can find what are the top 20 ticket infractions frequency-wise and revenue-wise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EBH7oWwkX0D"
      },
      "source": [
        "### 3.1.1 Top 20 ticket infractions - Frequency-wise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXvOgrLKI7kQ"
      },
      "source": [
        "#Top 20 ticket infractions - Frequency-wise\n",
        "df.infraction_code.value_counts().head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEOzwwztMG-b"
      },
      "source": [
        "### 3.1.2 Top 20 ticket infractions - Revenue-wise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMis0oy0I7go"
      },
      "source": [
        "# Top 20 ticket infractions - Revenue-wise\n",
        "\n",
        "table = pd.pivot_table(df, values='set_fine_amount', index=['infraction_code'], aggfunc=np.sum)\n",
        "table.rename(columns={'set_fine_amount':'revenue'}, inplace=True)\n",
        "table.sort_values(by='revenue', ascending=False).head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNMTusreMknh"
      },
      "source": [
        "### 3.1.3 Total Revenue Generated from all the tickets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljj24x4IMqsy"
      },
      "source": [
        "# Calculationg the total revenue generated from all the tickets\n",
        "print('Total Revenue Generated =',table.revenue.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTOeyIPqkjwH"
      },
      "source": [
        "### 3.1.4a Distance to closest parking lots for top 20 infractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iCnhHLykuQ6"
      },
      "source": [
        "#### Finding top 20 Infraction locations\n",
        "\n",
        "Creating a new column 'Location' with combination of location2 and location4 of Parking Infractions data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsFTArHHYHFl"
      },
      "source": [
        "# Defining function for combining location 2 and location 4\n",
        "\n",
        "# Please note that we have already cleaned data for location3 so that we can use that information to combine location2 and location4\n",
        "def full_loc(loc2, loc3, loc4):\n",
        "  loc=''\n",
        "  if loc3 == \"\":                        #If no data is available in location3\n",
        "    loc = str(loc2)                     #then location will be equal to location 2\n",
        "  else:                                 #otherwise\n",
        "    loc = str(loc2) + ', ' + str(loc4)  #location will be combination of location 2 and location 4\n",
        "  return loc + ', Toronto, ON'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4HHWR3uYHCS"
      },
      "source": [
        "# applying function full_loc to Parking Infraction dataframe\n",
        "# This cell will take some time to run\n",
        "df['location']=df.apply(lambda x: full_loc(x['location2'], x['location3'], x['location4']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYFfiJGzV7Vz"
      },
      "source": [
        "# Viewing data\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MltsnvVLV7Pn"
      },
      "source": [
        "# Finding top 20 infraction locations\n",
        "x = df.location.value_counts().head(20)\n",
        "top_20_loc = x.to_frame(name='value_counts')\n",
        "top_20_loc.reset_index(inplace=True)\n",
        "top_20_loc.rename(columns={'index':'infraction_location'}, inplace=True)\n",
        "top_20_loc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50plQWvMm9nx"
      },
      "source": [
        "#### Finding Latitude and Longitude of top 20 infraction location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu4Z_PcFV7SM"
      },
      "source": [
        "# Using Geocode to find latitude and longitude of a descriptive location\n",
        "locator = Nominatim(user_agent=\"myGeocoder\")\n",
        "\n",
        "def find_loc(loc):\n",
        "  location = locator.geocode(loc)\n",
        "  if location:\n",
        "    return (location.latitude, location.longitude)\n",
        "  else:\n",
        "    return (np.nan, np.nan)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXQxdO_YV7Mc"
      },
      "source": [
        "# Applying function find_loc to our data\n",
        "top_20_loc['lats']=top_20_loc.apply(lambda row: find_loc(row['infraction_location']), axis=1)\n",
        "top_20_loc[['latitude','longitude']] = pd.DataFrame(top_20_loc['lats'].tolist(), index=top_20_loc.index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaFf0jW2V7Dj"
      },
      "source": [
        "# Now we have the latitude and longitude information which can further be used to find distance from nearest Green Park Location\n",
        "top_20_loc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2_qyYwyuBaX"
      },
      "source": [
        "# Writing a function to find nearest parking location\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "def find_nearest_parking(infraction_location, list_of_carparks):\n",
        "  dist = list_of_carparks.apply(lambda x: geodesic(infraction_location, (x['lat'], x['lng'])).m, axis=1) #Creating list of distance of our point from all parking lots\n",
        "  nearest_parking_dist = dist.min()                                                                      # Finding parking lot with minimum distance\n",
        "  nearest_parking_address = list_of_carparks.iloc[dist.idxmin()]['address']                              # Finding address of parking lot with minimum distance\n",
        "  return nearest_parking_dist, nearest_parking_address\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBOv7L1OuBXU"
      },
      "source": [
        "# Applying function find_nearest_parking to our dataset\n",
        "nearest_parking = top_20_loc.apply(lambda x: find_nearest_parking(x['lats'], green_park), axis = 1)\n",
        "top_20_loc[['nearest_parking_dist','nearest_parking_address']] = pd.DataFrame(nearest_parking.tolist(), index=top_20_loc.index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIFpYtyVoLTc"
      },
      "source": [
        "#### Table containing information of distance of closest parking lots from top 20 infraction locations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxROoESzuBUb"
      },
      "source": [
        "top_20_loc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7eMR1ZToyjY"
      },
      "source": [
        "### 3.1.4b Distance of Closest TTC stop for top 20 infraction locations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKpNdKwiRSZs"
      },
      "source": [
        "# Creating finction to find nearest TTC stop\n",
        "def find_nearest_ttc(infraction_location, list_of_ttc):\n",
        "  dist = list_of_ttc.apply(lambda x: geodesic(infraction_location, (x['stop_lat'], x['stop_lon'])).m, axis=1)\n",
        "  nearest_ttc_dist = dist.min()\n",
        "  nearest_ttc_address = list_of_ttc.iloc[dist.idxmin()]['stop_name']\n",
        "  return nearest_ttc_dist, nearest_ttc_address"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTNtBI3WRSW8"
      },
      "source": [
        "#Applying function find_nearest_ttc\n",
        "nearest_ttc = top_20_loc.apply(lambda x: find_nearest_ttc(x['lats'], ttc), axis = 1)\n",
        "top_20_loc[['nearest_ttc_dist','nearest_ttc']] = pd.DataFrame(nearest_ttc.tolist(), index=top_20_loc.index)\n",
        "top_20_loc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li9iEGlci6_O"
      },
      "source": [
        "So Finally we have our data of top 20 infraction locations with nearest park location and nearest TTC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD038FFR6qa9"
      },
      "source": [
        "### 3.1.5a Impact of day of week on all infractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqvZDSMH6ozb"
      },
      "source": [
        "#First we are grouping data by day of week and infraction code to get desired result\n",
        "dow = df['set_fine_amount'].groupby([df.infraction_code, df.date_of_infraction.dt.weekday]).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM1so-IV6owp"
      },
      "source": [
        "# The result comes out as a multi index Series. Converting that multiindex Series to Datframe\n",
        "dow_impact = dow.unstack(level=1,fill_value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS_DfVbB6ot9"
      },
      "source": [
        "# Renaming columns to make data more readable\n",
        "dow_impact.rename(columns={0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'},inplace=True)\n",
        "dow_impact"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tef1eedkjxTR"
      },
      "source": [
        "from this table we can find frequency of a particular infraction code for any day of the week"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpkR3wJMkBpc"
      },
      "source": [
        "**If we want to visualize the impact of week on any infraction code we can do so by plotting for particular infraction code as below. Here we can even show multiple plots if required**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYWJ6vYL_X32"
      },
      "source": [
        "# By changing the value of n we can see impact of weekday for any infraction code. Here n denotes infraction code\n",
        "n = 5\n",
        "dow_impact.loc[n].plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gj6CqX5_Jej"
      },
      "source": [
        "### **3.1.5b Impact of month on all infractions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJYjSKo5kxJR"
      },
      "source": [
        "Similar to week we can analyze our data to see impact of month on frequency and type of infractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a7bAgfA6oqG"
      },
      "source": [
        "#grouping data by infraction code and month\n",
        "mnth = df['set_fine_amount'].groupby([df.infraction_code, df.date_of_infraction.dt.month]).count()\n",
        "mnth_impact = mnth.unstack(level=1,fill_value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRJTy_Y-6olx"
      },
      "source": [
        "# Renaming columns to make our data more readable\n",
        "mnth_impact.rename(columns={1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'},inplace=True)\n",
        "mnth_impact"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlrbEokd6oYh"
      },
      "source": [
        "# By changing the value of n we can see impact of month for any infraction code. Here n denotes infraction code\n",
        "n = 5   #Change value of n to look for a particular infraction code\n",
        "mnth_impact.loc[n].plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frxcGe1wpjEp"
      },
      "source": [
        "### 3.3.1.a.i Distributions of infractions by year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJvTrA9XG6js"
      },
      "source": [
        "# Grouping data by year and infraction code\n",
        "yr = df['set_fine_amount'].groupby([df.infraction_code, df.date_of_infraction.dt.year]).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNkR_3HAG6fC"
      },
      "source": [
        "yr_impact = yr.unstack(level=1,fill_value=0)\n",
        "yr_impact"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h5UPwXMG6ZL"
      },
      "source": [
        "yr_impact['total']=yr_impact[2016] + yr_impact[2017] + yr_impact[2018]\n",
        "yr_impact.sort_values(by=['total'], ascending=False, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "594FJHv3lnFY"
      },
      "source": [
        "Plotting data for only top 20 infraction codes as plotting for all infraction codes will make the visualization messy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNo1Gr1IODH7"
      },
      "source": [
        "yr_20 = yr_impact.head(20)\n",
        "yr_20.drop(columns=['total'], inplace=True)\n",
        "yr_impact_20 = yr_20.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB9ssvyyODE_"
      },
      "source": [
        "# Viewing data yearwise\n",
        "fig, ax=plt.subplots(3)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "fig.suptitle('Year Comparison', fontsize=20)\n",
        "yr_20[2016].plot.bar(ax=ax[0], color='r')\n",
        "yr_20[2017].plot.bar(ax=ax[1], color='g')\n",
        "yr_20[2018].plot.bar(ax=ax[2], color='b')\n",
        "\n",
        "ax[0].set_title('2016')\n",
        "ax[1].set_title('2017')\n",
        "ax[2].set_title('2018')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xHvpX6mVvsM"
      },
      "source": [
        "# plotting Distribution of infractions by year\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Year Impact', fontsize=20)\n",
        "yr_impact_20.plot.bar(subplots=True, layout=(4,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNzKaNHoRBWW"
      },
      "source": [
        "### 3.3.1.a.ii  Distribution of Infractions by Month\n",
        "\n",
        "Similar to year, we can visualize the same data month-wise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um31oiN0ODBr"
      },
      "source": [
        "mnth_impact['total'] = mnth_impact['Jan'] + mnth_impact['Feb'] + mnth_impact['Mar'] + mnth_impact['Apr'] + mnth_impact['May'] + mnth_impact['Jun'] + mnth_impact['Jul'] + mnth_impact['Aug'] + mnth_impact['Sep'] + mnth_impact['Oct'] + mnth_impact['Nov'] + mnth_impact['Dec']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cpt7RfUOC-4"
      },
      "source": [
        "mnth_impact.sort_values(by=['total'], ascending=False, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukR1gKamOC7v"
      },
      "source": [
        "mnth_impact_20 = mnth_impact.head(20)\n",
        "mnth_impact_20.drop(columns=['total'], inplace=True)\n",
        "m_temp = mnth_impact_20.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-skKykdTU5M"
      },
      "source": [
        "m_temp.plot.bar(subplots=True, layout=(4,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xeCLFlkX6Q5"
      },
      "source": [
        "### 3.3.1.b Distribution of top 20 infractions by fines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtOHa_RCRSAv"
      },
      "source": [
        "# Grouping data by infraction code and finding sum of fines\n",
        "fines = df['set_fine_amount'].groupby(df.infraction_code).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be8pHdmxZC1N"
      },
      "source": [
        "fines.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnzHsypmZCyu"
      },
      "source": [
        "fines.sort_values(ascending=False, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXlcB2EtZCwh"
      },
      "source": [
        "#Creating subset of top 20 infractions\n",
        "fines_20 = fines.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QNBey9wZCuy"
      },
      "source": [
        "# Plotting our data\n",
        "fines_20.plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za-aW0EWeWz1"
      },
      "source": [
        "### 3.3.2a Geographic distribution of top 20 infractions\n",
        "\n",
        "Sorry but I have not worked much with geospatial data before so I was unable to do this in given time frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-zOgzW4m5d1"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Get the dataset metadata by passing package_id to the package_search endpoint\n",
        "# For example, to retrieve the metadata for this dataset:\n",
        "\n",
        "url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca/api/3/action/package_show\"\n",
        "params = { \"id\": \"5e7a8234-f805-43ac-820f-03d7c360b588\"}\n",
        "package = requests.get(url, params = params).json()\n",
        "print(package[\"result\"])\n",
        "\n",
        "\n",
        "for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
        "    if resource[\"datastore_active\"]:\n",
        "        url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca/api/3/action/datastore_search\"\n",
        "        p = { \"id\": resource[\"id\"] }\n",
        "        data = requests.get(url, params = p).json()\n",
        "        df_geo = pd.DataFrame(data[\"result\"][\"records\"])\n",
        "        break\n",
        "df_geo.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlt7Nll6y5ez"
      },
      "source": [
        "xy = top_20_loc.longitude.mean()\n",
        "xy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgnDRzsSniMq"
      },
      "source": [
        "fig1 = plt.figure(figsize=(20,10))\n",
        "\n",
        "map = Basemap(projection='aeqd',\n",
        "              lon_0 = -79.40,\n",
        "              lat_0 = 43.68,\n",
        "              width = 1000000,\n",
        "              height = 1000000)\n",
        "\n",
        "map.drawmapboundary(fill_color='aqua')\n",
        "#Fill the continents with the land color\n",
        "map.fillcontinents(color='coral',lake_color='aqua')\n",
        "map.drawcoastlines()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy-8LiNS11Hr"
      },
      "source": [
        "data = go.Scattergeo(lon = top_20_loc['longitude'], lat = top_20_loc['latitude'], mode='markers', marker = dict(symbol = 'star',size=5,colorscale = 'Reds'))\n",
        "layout = dict(title = 'Green Park Locations')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hudF0wFt3nAj"
      },
      "source": [
        "choromap = go.Figure(data = [data],layout = layout)\n",
        "iplot(choromap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xMcpmGo3m9m"
      },
      "source": [
        "city_wards = pd.read_csv('/content/drive/My Drive/SDA/City Wards Data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79PoB1jU3m6k"
      },
      "source": [
        "city_wards.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Ym3ogd3m3b"
      },
      "source": [
        "# city_wards.geometry.plot(figsize=(40,20))\n",
        "# plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ7aJsh09IvS"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rulQIVzS9gJN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr0DE18z9_KG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4B1kvLp-BCw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X8b9Fy1_r5L"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmm6YPmNa1_C"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}